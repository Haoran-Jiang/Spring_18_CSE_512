{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we start, we need to import some necessary packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import KFold"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q1 Multinomial Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code serves data import and one hot encoder for our label variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Import data\n",
    "data1 = pd.read_csv('datasethw2/data1.txt', sep = '\\t', header= None)\n",
    "data2 = pd.read_csv('datasethw2/data2.txt', sep = '\\t', header = None)\n",
    "data3 = pd.read_csv('datasethw2/data3.txt', sep = '\\t', header = None)\n",
    "data = pd.concat([data1, data2, data3])\n",
    "\n",
    "# Shuffle our data\n",
    "np.random.seed(0)\n",
    "data = data.sample(frac = 1).reset_index(drop = True)\n",
    "\n",
    "# One hot encoder\n",
    "onehot_encoder = OneHotEncoder(sparse=False)\n",
    "all_x = data.iloc[:,:-1]\n",
    "all_y = onehot_encoder.fit_transform(data[5].values.reshape(-1,1))\n",
    "\n",
    "# Dimension \n",
    "n_x = all_x.shape[1] # Features Number\n",
    "n_y = all_y.shape[1] # Class Number"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, we set up our TensorFlow Multinomial Classification model. Here, we use the cross-entropy as our cost function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Y = tf.placeholder(tf.float32, shape = [None, n_y])\n",
    "X = tf.placeholder(tf.float32, shape = [None, n_x])\n",
    "\n",
    "W = tf.Variable(tf.random_uniform([n_x,n_y]))\n",
    "b = tf.Variable(tf.random_uniform([n_y]))\n",
    "\n",
    "# Soft-Max Prediction Outcome\n",
    "prediction = tf.nn.softmax(tf.matmul(X, W) + b)\n",
    "\n",
    "# Cross-entropy Cost Function\n",
    "cost = tf.reduce_mean(\\\n",
    "                      -tf.reduce_sum(Y * tf.log(prediction), axis = 1))\n",
    "\n",
    "# Accuracy\n",
    "accuracy = \\\n",
    "tf.reduce_mean(\\\n",
    "               tf.cast(\\\n",
    "                       tf.equal(tf.argmax(Y,1),tf.argmax(prediction,1)),\\\n",
    "                       tf.float32))\n",
    "\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate=0.01).minimize(cost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "accuracy_record = np.zeros(10)\n",
    "W_record = np.zeros([n_x,n_y,10])\n",
    "b_record = np.zeros([n_y,10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "for step in range(10):\n",
    "    train_x, test_x, train_y, test_y = train_test_split(all_x, all_y, \n",
    "                                                        test_size = 0.2)\n",
    "    for epoch in range(10000):\n",
    "        sess.run([optimizer], feed_dict= {X: train_x, \n",
    "                                          Y: train_y})\n",
    "    W_hat, b_hat = sess.run([W,b])\n",
    "    W_record[:,:,step] = W_hat\n",
    "    b_record[:,step] = b_hat\n",
    "    accuracy_record[step] = sess.run(accuracy, \n",
    "                                     feed_dict = {X:test_x, Y: test_y}) \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy \n",
      " [ 0.75        0.78333336  0.81666666  0.85000002  0.80000001  0.85000002\n",
      "  0.78333336  0.83333331  0.85000002  0.73333335]\n"
     ]
    }
   ],
   "source": [
    "print('Accuracy','\\n',accuracy_record)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best W: \n",
      " [[ 0.258131    0.68533254  0.16778211]\n",
      " [ 0.15310754  0.45301342  0.36581433]\n",
      " [-0.83655965  1.44814765 -0.18005736]\n",
      " [-0.75564283  1.22818983  0.68093216]\n",
      " [-0.01445479  0.51505649  0.46779269]] \n",
      " Best b \n",
      " [ 5.62745571 -5.59471416  2.40941286]\n"
     ]
    }
   ],
   "source": [
    "print('Best W:','\\n',\n",
    "      W_record[:,:,np.argmax(accuracy_record)],'\\n',\n",
    "     'Best b', '\\n',\n",
    "      b_record[:,np.argmax(accuracy_record)])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q2 Multinomial Classification & Feature Reduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use the 5-fold cross validation to find the best 4 features. First, we use our training data, and randomly divide them into 5 folds. At fold i, we use other 4 folds data as our training data, and test our model in fold i data. The feature selection metric is the average accuracy of 5-fold cross validation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 311,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(0)\n",
    "train_x, test_x, train_y, test_y = \\\n",
    "train_test_split(all_x, all_y, test_size = 0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 313,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y = tf.placeholder(tf.float32, shape = [None, n_y])\n",
    "X = tf.placeholder(tf.float32, shape = [None, n_x-1])\n",
    "\n",
    "W = tf.Variable(tf.random_uniform([n_x-1,n_y]))\n",
    "b = tf.Variable(tf.random_uniform([n_y]))\n",
    "\n",
    "# Soft-Max Prediction Outcome\n",
    "prediction = tf.nn.softmax(tf.matmul(X, W) + b)\n",
    "\n",
    "# Cross-entropy Cost Function\n",
    "cost = tf.reduce_mean(\n",
    "    -tf.reduce_sum(Y * tf.log(prediction), axis = 1))\n",
    "\n",
    "# Accuracy\n",
    "accuracy = \\\n",
    "tf.reduce_mean(tf.cast(tf.equal(tf.argmax(Y,1),\\\n",
    "                     tf.argmax(prediction,1)),\\\n",
    "                       tf.float32))\n",
    "\n",
    "optimizer = \\\n",
    "tf.train.GradientDescentOptimizer(learning_rate=0.01).minimize(cost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 315,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 0\n",
    "accuracy_record = np.zeros([5,5])\n",
    "kf = KFold(n_splits = 5, shuffle = False,\n",
    "               random_state = 1)\n",
    "for train_index, test_index in kf.split(train_x):\n",
    "    cv_train_x, cv_test_x = \\\n",
    "    train_x.iloc[train_index,:],train_x.iloc[test_index,:]\n",
    "    cv_train_y, cv_test_y = \\\n",
    "    train_y[train_index], train_y[test_index]\n",
    "    for i in range(5):\n",
    "        sess = tf.Session()\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        for epoch in range(10000):\n",
    "            sess.run([optimizer], \n",
    "                     feed_dict= \\\n",
    "                     {X: cv_train_x.drop(cv_train_x.columns[[i]], axis=1, inplace=False),\\\n",
    "                      Y: cv_train_y})\n",
    "        accuracy_record[k,i] = \\\n",
    "        sess.run(accuracy,feed_dict ={X:cv_test_x.drop(cv_test_x.columns[[i]],\\\n",
    "                                                       axis=1, inplace=False),\\\n",
    "                                      Y: cv_test_y})\n",
    "        # accuracy_record[k,i] means accuracy of model which excludes i-th column at kth fold data\n",
    "    k = k+1\n",
    "    \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 324,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.75833333,  0.77916669,  0.75      ,  0.73333334,  0.77500001])"
      ]
     },
     "execution_count": 324,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_record\n",
    "np.mean(accuracy_record, axis = 0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The best four features model includes variable 1, varialbe 3, variable 4, variable 5 since this model has the highest accuracy in 5-fold cross validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 307,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_record = np.zeros(10)\n",
    "W_record = np.zeros([n_x-1,n_y,10])\n",
    "b_record = np.zeros([n_y,10])\n",
    "\n",
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "for step in range(10):\n",
    "    train_x, test_x, train_y, test_y = train_test_split(all_x, all_y, test_size = 0.2)\n",
    "    for epoch in range(10000):\n",
    "        sess.run([optimizer],\n",
    "                 feed_dict= \\\n",
    "                 {X: train_x.drop(train_x.columns[[1]], \\\n",
    "                                  axis = 1,inplace = False),\\\n",
    "                  Y: train_y})\n",
    "    W_hat, b_hat = sess.run([W,b])\n",
    "    W_record[:,:,step] = W_hat\n",
    "    b_record[:,step] = b_hat\n",
    "    accuracy_record[step] = sess.run(accuracy,\\\n",
    "                                     feed_dict = \\\n",
    "                                     {X:test_x.drop(test_x.columns[[4]],\\\n",
    "                                                    axis = 1,inplace = False),\\\n",
    "                                      Y: test_y}) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 308,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy \n",
      " [ 0.43333334  0.5         0.48333332  0.55000001  0.44999999  0.56666666\n",
      "  0.53333336  0.38333333  0.5         0.43333334]\n"
     ]
    }
   ],
   "source": [
    "print('Accuracy','\\n',accuracy_record)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best W: \n",
      " [[ 0.53749949  0.97714525  0.51486504]\n",
      " [-0.53402317  1.85327792  0.37368262]\n",
      " [-0.13448872  1.42349064  1.08562398]\n",
      " [ 0.37720126  0.75136161  0.70024669]] \n",
      " Best b \n",
      " [ 4.51101494 -4.61911535  1.93547416]\n"
     ]
    }
   ],
   "source": [
    "print('Best W:','\\n',W_record[:,:,np.argmax(accuracy_record)],'\\n',\n",
    "     'Best b', '\\n',b_record[:,np.argmax(accuracy_record)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (C)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We checked the average accuracy of Q2 is 0.8083 while the average accuracy of Q1 is 0.8050. The best four features model includes variable 1, varialbe 3, variable 4, variable 5 since this model has the highest accuracy in 5-fold cross validation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q3 SVM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we use GridSeach Cross Validation to do tuning our SVM models. The hyperparameters we want to tune is following: Kernel type (Linear and Radial), Cost C, and Gamma for Radial Kernel Basis. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 325,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.svm import SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 326,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_x, test_x, train_y, test_y = \\\n",
    "train_test_split(all_x,data[5], test_size = 0.5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 331,
   "metadata": {},
   "outputs": [],
   "source": [
    "tuned_parameters = [{'kernel': ['rbf'], 'gamma': [1,1e-2,1e-3, 1e-4],\n",
    "                     'C': [1, 10, 100, 1000]},\n",
    "                    {'kernel': ['linear'], 'C': [1e-1,1, 10, 100, 1000]}]\n",
    "\n",
    "scores = ['accuracy']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 332,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Tuning hyper-parameters for accuracy\n",
      "\n",
      "Best parameters set found on development set:\n",
      "\n",
      "{'C': 1, 'kernel': 'linear'}\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "0.720 (+/-0.179) for {'C': 1, 'gamma': 1, 'kernel': 'rbf'}\n",
      "0.793 (+/-0.152) for {'C': 1, 'gamma': 0.01, 'kernel': 'rbf'}\n",
      "0.473 (+/-0.093) for {'C': 1, 'gamma': 0.001, 'kernel': 'rbf'}\n",
      "0.373 (+/-0.039) for {'C': 1, 'gamma': 0.0001, 'kernel': 'rbf'}\n",
      "0.713 (+/-0.125) for {'C': 10, 'gamma': 1, 'kernel': 'rbf'}\n",
      "0.813 (+/-0.066) for {'C': 10, 'gamma': 0.01, 'kernel': 'rbf'}\n",
      "0.780 (+/-0.169) for {'C': 10, 'gamma': 0.001, 'kernel': 'rbf'}\n",
      "0.473 (+/-0.093) for {'C': 10, 'gamma': 0.0001, 'kernel': 'rbf'}\n",
      "0.713 (+/-0.125) for {'C': 100, 'gamma': 1, 'kernel': 'rbf'}\n",
      "0.760 (+/-0.122) for {'C': 100, 'gamma': 0.01, 'kernel': 'rbf'}\n",
      "0.807 (+/-0.171) for {'C': 100, 'gamma': 0.001, 'kernel': 'rbf'}\n",
      "0.760 (+/-0.180) for {'C': 100, 'gamma': 0.0001, 'kernel': 'rbf'}\n",
      "0.713 (+/-0.125) for {'C': 1000, 'gamma': 1, 'kernel': 'rbf'}\n",
      "0.740 (+/-0.092) for {'C': 1000, 'gamma': 0.01, 'kernel': 'rbf'}\n",
      "0.807 (+/-0.068) for {'C': 1000, 'gamma': 0.001, 'kernel': 'rbf'}\n",
      "0.807 (+/-0.171) for {'C': 1000, 'gamma': 0.0001, 'kernel': 'rbf'}\n",
      "0.813 (+/-0.151) for {'C': 0.1, 'kernel': 'linear'}\n",
      "0.833 (+/-0.059) for {'C': 1, 'kernel': 'linear'}\n",
      "0.800 (+/-0.111) for {'C': 10, 'kernel': 'linear'}\n",
      "0.800 (+/-0.111) for {'C': 100, 'kernel': 'linear'}\n",
      "0.800 (+/-0.111) for {'C': 1000, 'kernel': 'linear'}\n",
      "\n",
      "Detailed classification report:\n",
      "\n",
      "The model is trained on the full development set.\n",
      "The scores are computed on the full evaluation set.\n",
      "\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          1       0.81      0.91      0.86        56\n",
      "          2       0.80      0.83      0.81        47\n",
      "          3       0.76      0.62      0.68        47\n",
      "\n",
      "avg / total       0.79      0.79      0.79       150\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for score in scores:\n",
    "    print(\"# Tuning hyper-parameters for %s\" % score)\n",
    "    print()\n",
    "\n",
    "    clf = GridSearchCV(SVC(), tuned_parameters, cv=5,\n",
    "                       scoring= score)\n",
    "    clf.fit(train_x, train_y)\n",
    "\n",
    "    print(\"Best parameters set found on development set:\")\n",
    "    print()\n",
    "    print(clf.best_params_)\n",
    "    print()\n",
    "    print(\"Grid scores on development set:\")\n",
    "    print()\n",
    "    means = clf.cv_results_['mean_test_score']\n",
    "    stds = clf.cv_results_['std_test_score']\n",
    "    for mean, std, params in zip(means, stds, clf.cv_results_['params']):\n",
    "        print(\"%0.3f (+/-%0.03f) for %r\"\n",
    "              % (mean, std * 2, params))\n",
    "    print()\n",
    "\n",
    "    print(\"Detailed classification report:\")\n",
    "    print()\n",
    "    print(\"The model is trained on the full development set.\")\n",
    "    print(\"The scores are computed on the full evaluation set.\")\n",
    "    print()\n",
    "    y_true, y_pred = test_y, clf.predict(test_x)\n",
    "    print(classification_report(y_true, y_pred))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
